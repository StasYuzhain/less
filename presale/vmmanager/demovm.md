---
sidebar_position: 1
---
# Сценарий показа

Здравствуйте, коллеги. Меня зовут ...., я Пресэйл Инженер компании ISPsystem. Сегодня я покажу вам наш продукт VMmanager.

Посмотрим что это за платформа, что она позволяет делать, какие задачи решает.

VMmanager - это платформа виртуализации которая позволяет управлять виртуальной инфраструктурой из единого интерфейса, как аппаратной на базе QEMU-KVM так и контейнерной на базе LXD LXC.

Это полностью наша разработка. На данный момент актуальна 6 версия продукта.
Продукт имеет микро сервисную архитектуру, под капотом порядка 30 сервисов которые взаимодействуют между собой посредством API (Open API и REST API). Все действия, которые можно делать из веб интерфейса, можно сделать по API.

В продукте используется встроенный авторизированный Swagger, который позволяет протестировать работу API-запроса. Используя такие запросы, можно писать собственные интеграции и обслуживать инфраструктуру, встраивая инструмент в бизнес процессы компании.

VMmanager позволяет масштабировать инфраструктуру, создавать кластеры, которые могут быть так же и отказоустойчивыми, с автоматической миграцией, эвакуацией виртуальных машин, в случае аварии на одном из узлов кластера.

Имеется поддержка VxLanов – это виртуальные приватные частные сети которые позволяют объединять виртуальные машины как в пределах одного кластера, так и из разных кластеров и из кластеров расположенных разных дата-центрах. Даже если разные сегменты имеется возможность объединять виртуальные машины одного пользователя в такие VxLanы.

Есть поддержка сетевых хранилищ как программно определяемых Ceph, так же любые SAN хранилища, SAN может быть реализован на основе протоколов FCP (Fibre Channel Protocol), iSCSI (Internet Small Computer System Interface), InfiniBand, FCoE (Fibre chanell over ethernet), сам VMmanager работает с блочным устройством подключённым к узлу, создает на этом устройстве LVM и управляет дисками через LVM.

Так же есть возможность предоставления сервисов по модели SaaS, в качестве услуги может быть на пример база данных, которую пользователь может заказать как услугу, пользователю не нужно вникать в настройки виртуальной машины установки ПО, по факту создается виртуальная машина, вероятнее на базе контейнерной виртуализации, разворачивается необходимая СУБД и пользователю отправляется только информация для подключения к базе данных.

Имеется портал самообслуживания, на который пользователи могут заходить под своими локальными или доменными учетными записями, поскольку имеется синхронизация с LDAP, FreeIPA.

Так же у нас имеется интеграция с Terraform , что позволяет реализовать подход инфраструктуры в виде кода (IaC). С помощью Terraform можно:
* Управлять пользователями, Группами пользователей
* Управлять как физическими так и виртуальными сетями
* Управлять всем жизненным циклом виртуальных объектов KVM (поддержка LXD пока не планируется)
 
Подход «инфраструктура как код» дает не только, гарантию повторяемого результата, но и также автоматическое описание и документирование инфраструктуры.

Ну и самая главная ценность здесь это - управление большим количеством виртуальных объектов. В рамках одного экземпляра VMmanager, мы можем обслуживать большое количество виртуальных машин. Тут без автоматизации не обойтись т.к через веб интерфейс, администратору, всем этим управлять будет довольно сложно.

Так же имеется интеграция с Zabbix, на сайте забикса https://www.zabbix.com/ru/integrations 
Это уже готовые шаблоны самого забикса. Используя шаблоны Zabbix, можно получить настроенную систему мониторинга без каких-либо дополнительных трудозатрат. Шаблон уже содержит: Триггеры, Виджеты, Автопоиск узлов и виртуальных машин.

Кроме этого, есть интеграция с Grafana и Telegram, более детально рассмотрим далее при демонстрации продукта.

Давайте перейдем непосредственно к интерфейсу самой платформы.
Это главный дашборд, на котором расположена статистика данные по кластерам колличестве узлов, ВМ. Есть информация о задачах за последние сутки, информация о загрузке сервера на который установлен сам VMmanager, данные по ядрам лицензии.

Ниже представлены виджеты на которых можно посмотреть утилизацию ресурсов, в разрезе узлов, виртуальных машин. Так же возможно добавить свои виджеты.
Виджеты отображаются с помощью графаны, возможно перейти в саму графану, здесь есть настроенные дашборды, ыв можете создавать свои, все метрики которые доступные в платформе ВМманаджер они так же доступны и сдесь
Сваггер 
Создание кластера Кластер - это такая абстрактная сущность, которая позволяет нам объединять узлы по каким-то общим признакам. 
Мастер добавления кластера разбит на 4 шага
Это Общие настройки где мы указываем тип используемой виртуализации, наименование, часовой пояс и ДНС сервера для виртуальных машин.
Следующий шаг это Доступы и ресурсы, где необходимо выбрать список операционных систем доступных для установки на виртуальные машины. Это шаблоны операционные системы из нашего репозитория которые доступны из коробки, чуть позже рассмотрим шаблоны более подробно.
Длалее разрешаем или запрещаем пользователям подключение по протоколу SPICE, загружать свои ISO-образы, подключать/отключать свои диски VM, изменять доменное имя. Указываем шаблон домена в котором будут создаваться виртуальные машины.
Так же на 2-м шаге возможно указать настройки распределения виртуальных машин на узлах кластера. Это коэффициенты оверселлинга (коэффициент переподписки) по CPUа так же по оперативной памяти. Возможно установить ограничение на количество виртуальных машин создаваемых в кластере, по умолчанию не органично. Далее выбираем тип распределения равномерное или последовательное, при равномерном распределении новые виртуальные машины будут создаваться на наименее загруженном узле кластера. При последовательном виртуальные машины полностью заполняют вначале один узел кластера затем другой
На шаге 3 настраиваем хранилища на узлах
можно изменить папки по умолчанию для хранения образов и резервных копий и хранения шаблонов операционных систем. это директории которые будут располагаться на физических хостах
При первом создании виртуальной машины образ из нашего репозитория загрузится в эту папку и далее будет использоваться локально.
Выбираем хранение дисков виртуальных машин, по умолчанию используется локальное хранилище - это локальные диски узлов. Возможно добавить любое другое Поддерживаются сетевые и локальные хранилища. Из локальных это - LVM или файловое хранилище. Также можно добавить сетевое хранилище это - Ceph, SAN. Можно добавлять множество хранилищ на один кластер. Есть поддержка тегирования хранилищ  для использования в дальнейшем при создании дисков виртуальных машин.Кроме того, есть возможность мигрировать диски виртуальных машин из разных хранилищ. Например, диск из одного хранилища переместить в другое, средствами платформы из web-интерфейса. 
И на последнем шаге мы указываем типа настройки сети:
В VMmanager существует три типа сетевых настроек кластера: "Коммутация", "Маршрутизация" и "IP-fabric". Они отличаются порядком назначения IP-адресов виртуальным машинам и топологией построения сетей.
Коммутация - это стандартные Linux bridge.  Для кластера выделяется пул, объединяющий блоки IP-адресов из физических сетей. IP-адресами из этого пула вы можете управлять в разделе Сети. Для такого типа настройки доступна миграция виртуальных машин (ВМ) между узлами кластера.
Маршрутизация - это тоже на Linux bridge, но для определенного типа дата-центров когда на сетевом оборудовании дата-центра включена функция Port Security. Эта функция блокирует отправку кадров Ethernet, если MAC-адрес отправителя не указан как разрешённый. 
IP-Fabric - это возможность использовать BGP для того, чтобы информация об адресах виртуальных машин отправлялась посредством BGP, при этом узлы в кластере могут быть в серой сети.
Создание кластера "IP-fabric" возможно только в сети, поддерживающей маршрутизацию по протоколу iBGP. Передачу информации о маршрутах iBGP выполняет оборудование Route Reflector. В качестве такого оборудования могут использоваться физические или виртуальные маршрутизаторы и/или серверы.
Указать проверочный адрес, как правило это шлюз, чтобы каждый узел мог пинговать себя до шлюза, чтобы наши агенты понимали выпал этот узел из кластера или нет.
Седующиее это виртуальные сети в кластере возможно включить VxLan имеется 2 варианта это либо Fullmesh подключение по типу звезда между узлами используется широковещательный трафик но при этом ничего настраивать не нужно, если в кластере больше чем 10 узлов, то необходимо Route Reflector, который возможно установить автоматически скриптом при создании виртуальной машины.

После тако как мы создали кластер, необходимо добавить узел. Для этого на нем должна быть установлена чистая система, рекомендуется ее обновить для кластера с типом виртуализации KVM поддерживается Астра линукс 1.7 Альмалинукс 8, Убунту 20,04 для кластера с типом виртуализации LXD Ubuntu 20.04/
Вводим название узла, выбираем кластер в который добавляем узел. ВМменеджер автоматически настраивает сеть, ИП адрес должен быть статический прописанный в ручную. Сеть возможно не настраивать автоматически, если вы планируйте руками настроить .
Указываем адрес узла, пароль для подключения по ssh.
Так же возможно настроить распределение ВМ на конкретном узле, информация беретя из настроек кластера.
После добавления узла в кластер отобразится в списке узлов. Здесь отображается информация по утилизации ресурсов каждого узла, информация по количеству виртуальных машин
В меню нам доступно информация, можно запретить создание виртуальных машин на время каких-то технических работ, изменить параметры подключения, изменить фильтры распределения
В настройках узла информация о загрузке, последние события на узле. Все события отображаются в История.
Есть статистика по ресурсам за определенный период
Дисковое пространство, видим какие виртуальные машины где располагаются, так же образы, шаблоны ОС и резервные копии. 
В настройках сети мы можем посмотреть схему, возможно создать дополнительный бридж указать тег влан подать порт  с транком с вланами просто создавать бриджи и прокидывать до виртуальных машин. Если у вас несколько физических интерфейсов, можно объединять их в бонды поддерживаются все стандартные режимы для линукса.
Список кластеров так же отображается утилизация, только суммарно по всем узлам. Можно видеть сколько узлов, виртуальных машин, настройки типа сети.
Какие настройки есть, их можно поменять. Настройки сети, если в кластере включены виртуальные сети это не значит что они доступны пользователю, это как отдельная услуга пользователю может быть предоставлена. Видим ВхЛаны и пользователя для которого они включены. Тут же их возможно создавать, мы выбираем пользователя указываем название указываем сеть возможно даже одинаковые у разных пользователей они ни как не пересекаются.
Далее у нас настройки хранилища, сколько места использовано, сколько виртуальных машин, так же здесь возможно изменить название, тэги. Так же выбрать его основным или выключить если не планируем использовать локальные хранилища.
Сетевые хранилища, так же можем сделать основным, тогда новые машины будут создаваться в нем. Тут е возможно добавить.
Отказоустойчивость, если она нам нужна мы можем ее включить, выбрать отдельные виртуальные машины которые будут мигрировать, это возможно использовать как отдельную услугу.
Далее перейдем к виртуальным машинам.
Здесь мы видим основную информацию по ВМ, нам доступна консоль VNC и SPICE, нам доступны  операции на виртуальной машиной, мы работаем в интерфейсе администратора нам доступен весь функционал Есть режим восстановления в этом случаее виртуальная машина подключится с шаблона диагностики. Есть миграция, миграция у нас живая как между узлами в рамках одного кластера так и между кластерами и между разными хранилищами
В карточке виртуальной машины Так же есть параметры, здесь более детальная информация об утилизации установленных лимитах, так же последние события.
По подробнее расскажу о лимитах, это тонкие настройки 
Возможно внести изменения на скорость входящего исходящего трафика, можно увеличить вес CPU либо вес операции ввода вывода – это приоритет доступа к физическим ресурсам 
Можно ограничить чтение запись, количество соединений есть антиспуфинг, так же заблокировать входящие /исходящие порты указанием диапазона. Так же настройка диска, мы можем указать не просто диск увеличить а конкретный раздел. И мы можем поменять тип эмуляции. По умолчанию у нас используется эмуляция Qemu это позволяет не привязываться к аппаратной архитектуре серверов, в этом случае миграция работает в независимости от того на какой узел виртуальная  машина переезжает.
Вкладка сети- физические сети
Здесь мы наши сети перечислить и контролировать выдачу  ип адресов из них
Мы можем выделить пулы адресов только для наших виртуальных машин
На вкладке IPадреса общий список выданных адресов. Так же есть сервис DNSBL для проверки нахождения адресов в черных списках.
Шаблоны операционных систем, здесь у нас шаблоны доступные из коробки, так же имеются репозитории, возможно добавлять свои. Так же здесь расположены конфиурации для создания виртуальных машин, возможно создать свои, и указать тонкие настройки.
Скрипты список скриптов доступных сразу из коробки среди этих скриптов есть Route Reflector про который я говорил его можно использовать для установки и настройки роутрефлектора.
Есть возможность создавать свои скрипты можем выбрать кто владелец, кому этот скрипт доступен, если мы даем доступ всем, мы можем скрыть содержание скрипта ели мы используем какие-то пароли или лицензионные ключи, которые мы не хотим показывать. Так же это возможно через локальные или глобальные переменные. Для линукс используются шел скрипты для виндовс PowerShell. С помощью тэгирования возможно ограничить где будет запущен скрипт. Мы можем добавлять параметры, которые будем запрашивать у пользователя перед выполнением скрипта,   
Имеется возможность результат работы скрипта отправить на емайл. Этот функциона используется для реализации модели SaaS.
Так же есть возможность использования скриптов для узлов. Либо шел либо ансибэл.
Переменные для скриптов вынесены в отдельную вкладку, так же возможно создавать свои переменные возможно скрывать значение переменной.
Давайте посмотрим процесс создания виртуальной машины.
Виртуальная машина создается из шаблона либо из образа.
Рассмотрим создание на базе наших шаблонов. Выбираем кластер, здесь будет отображаться список всех доступных кластеров они сортируются по частоте использования, сразу отображаются тип сети и доступность ресурсов. Если ресурсов будет недостаточно Вмменеджер об этом предупредит. Выбираем операционную систему, версию которую мы хотим установить, они так же сортируются по частоте использования. Либо открыть полный список и выбрать какою-то определенную. Поле того как выбрали операционную систему
